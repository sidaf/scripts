#!/usr/bin/env ruby
#/ Usage: web_brute [options] ...
#/
#/ This script ...

$stderr.sync = true

require 'optparse'
require 'colorize'
require 'typhoeus'
require 'digest'

METHOD = 'GET'
HOST = nil
DIR_LIST = File.expand_path(File.dirname(__FILE__) + "/wordlists/web/raft-large-directories-lowercase.txt")
FILE_LIST = File.expand_path(File.dirname(__FILE__) + "/wordlists/web/raft-large-files-lowercase-noext.txt")
USER_AGENTS = File.readlines(File.expand_path(File.dirname(__FILE__) + "/wordlists/user_agents.txt")).each {|line| line.chomp!}
MAX_CONCURRENCY = 100
STATUS_CODES = [200, 301, 307, 401]
IGNORE_STATUS_CODES = [404,500]
IGNORE_CONTENT_LENGTH = []
RECURSE = true
RECURSE_DEPTH = 5
CUSTOM_EXTENSIONS = ['htm', 'html', 'asp', 'aspx', 'jsp', 'php']
VARIANT_EXTENSIONS = ['bac', 'BAC', 'backup', 'BACKUP', 'bak', 'BAK', 'bk', 'conf', 'cs', 'csproj', 'gz', 'inc', 'INC', 'java', 'log', 'lst', 'old', 'OLD', 'orig', 'ORIG', 'sav', 'save', 'tar', 'temp', 'tmp', 'TMP', 'vb', 'vbproj', 'zip', '$$$', '-OLD', '-old', '0', '1', '~1', '~bk']
IGNORE_EXTENSIONS = ['7z', 'aac', 'bz2', 'class', 'com', 'dmg', 'doc', 'docx', 'exe', 'gif', 'gz', 'iso', 'jar', 'jpeg', 'jpg', 'mp3', 'mpeg', 'mpg', 'pdf', 'pif', 'png', 'ram', 'rar', 'scr', 'snp', 'swf', 'tgz', 'tif', 'tiff', 'wav', 'xls', 'xlsx', 'xml', 'zip']

# Functions

def brute(target, hostname, wordlist)
    results = SortedSet.new

    Typhoeus::Config.user_agent = USER_AGENTS.sample
    hydra = Typhoeus::Hydra.new(max_concurrency: MAX_CONCURRENCY)

    uri = URI(target)
    vhost = hostname || uri.host
    resolve = Ethon::Curl.slist_append(nil, "#{vhost}:#{uri.port}:#{uri.host}")

    wordlist.each do |word|
        url = "#{target.chomp('/')}/#{word}"

        request = Typhoeus::Request.new(
            url,
            resolve: resolve,
            method: METHOD,
            followlocation: false,
            connecttimeout: 15,
            timeout: 20,
            ssl_verifyhost: 0,
            ssl_verifypeer: false
        )

        request.on_complete do |response|
            if response.timed_out?
                STDERR.puts "#{url},TIMEOUT".red
            elsif response.code.zero?
                # Could not get an http response, something's wrong.
                STDERR.puts "#{url},ERROR".red
            else
                content_length = response.headers['content-length'].nil? ? response.body.size : response.headers['content-length']

                next unless STATUS_CODES.empty? or STATUS_CODES.include? response.code
                next if IGNORE_CONTENT_LENGTH.include? content_length
                next if IGNORE_STATUS_CODES.include? response.code

                hash = METHOD == 'GET' ? Digest::MD5.hexdigest(response.body) : String.new

                puts "#{url},#{METHOD},#{response.code},#{content_length},#{hash},#{vhost}"
                results << url
            end
        end

        hydra.queue request
    end

    hydra.run

    results
end

# argument default values 

target = nil

# parse arguments

file = __FILE__
ARGV.options do |opts|
    opts.on("-t", "--target URL", String) { |val| target = val }
    opts.on_tail("-h", "--help")          { exec "grep ^#/<'#{file}'|cut -c4-" }
    opts.parse!
end


# check arguments

if target.nil? then
    puts ARGV.options
    exit 1
else
    target = target.chomp('/')
end

# 1) Test directory list

dir_results = SortedSet.new

dirs = IO.readlines(DIR_LIST, :encoding => 'ISO-8859-1').map do |word|
    word = word.strip                       # remove newline and whitespace characters
    word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
    word += '/' if word[-1,1] != '/'        # add trailing slash if it doesn't exist
end
dirs.uniq!

urls = SortedSet.new
urls << target
depth = 0

loop do
    results = SortedSet.new
    depth += 1

    urls.each do |url|
        STDERR.puts "[*] Testing directory list - #{url}".green
        results += brute(url, HOST, dirs)
    end

    break if results.empty?
    break unless RECURSE?
    break if depth == RECURSE_DEPTH

    dir_results += results
    urls = results
end

# 2) Test file list with no extensions

files = IO.readlines(FILE_LIST, :encoding => 'ISO-8859-1').map do |word|
    word = word.strip                       # remove newline and whitespace characters
    word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
    word = word.chomp('/')                  # remove trailing slash if it exists
    word
end
files.uniq!

dir_results << target

file_results = SortedSet.new

dir_results.each do |url|
    STDERR.puts "[*] Testing file list with no extensions - #{url}".green
    file_results += brute(url, HOST, files)
end

# 3) Test file list with custom extensions

files = IO.readlines(FILE_LIST, :encoding => 'ISO-8859-1').flat_map do |word|
    word = word.strip                       # remove newline and whitespace characters
    word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
    combi = Array.new
    CUSTOM_EXTENSIONS.each do |ext|
        combi << word + '.' + ext
    end
    combi
end
files.uniq!

dir_results.each do |url|
    STDERR.puts "[*] Testing file list with custom extensions - #{url}".green
    file_results += brute(url, HOST, files)
end

# 4) Test variant extensions on discovered files

files = SortedSet.new
file_results.each do |url|
    uri = URI(url)
    VARIANT_EXTENSIONS.each do |ext|
        file = uri.path + '.' + ext
        file = file[1..-1] if file[0,1] == '/'  # remove leading slash if it exists
        files << file
    end
end

STDERR.puts "[*] Testing variant extensions on discovered files".green
file_results += brute(target, HOST, files)


# 5) Test file list with observed extensions (if different from custom/variant), except for (bz2, class, com, doc, docx, exe, gif, gz, jar, jpeg, jpg, mp3, mpeg, mpg, pdf, pif, png, ram, rar, scr, snp, swf, tgz, tif, wav, xls, xlsx, xml, zip).
# 6) Test observed files with custom extensions.
# 7) Test observed files with variant extensions.
# 8) Test observed files with observed extensions, except for (bz2, class, com, doc, docx, exe, gif, gz, jar, jpeg, jpg, mp3, mpeg, mpg, pdf, pif, png, ram, rar, scr, snp, swf, tgz, tif, wav, xls, xlsx, xml, zip).

# Options:
# A) Extract links
# B) Files only, directories only, both files and directories
# C) Recurse sub-directories (to depth: X)

# Input: List of URLs

# Group URLs by IP + hostname, so that enumeration does not happen twice
# Split each batch of tests into tasks, and add to a queue
# Tasks can then be pushed to top or bottom of queue as appropiate