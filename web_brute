#!/usr/bin/env ruby
#/ Usage: web_brute [options] ...
#/
#/ This script ...

$stdout.sync = $stderr.sync = true

require 'optparse'
require 'colorize'
require 'typhoeus'
require 'digest'
require 'uri'


METHOD = 'GET'
HOST = nil
DIR_LIST = File.expand_path(File.dirname(__FILE__) + "/wordlists/web/raft-large-directories-lowercase.txt")
FILE_LIST = File.expand_path(File.dirname(__FILE__) + "/wordlists/web/raft-large-files-lowercase-noext.txt")
USER_AGENTS = File.readlines(File.expand_path(File.dirname(__FILE__) + "/wordlists/web/user_agents.txt")).each {|line| line.chomp!}
HTTP_404_SIGS = File.readlines(File.expand_path(File.dirname(__FILE__) + "/wordlists/web/404_signatures.txt")).each {|line| line.chomp!}
MAX_CONCURRENCY = 100
MAX_TIMEOUTS = 10
MAX_ERRORS = 10
REPORT_STATUS_CODES = [200, 401]#, 301, 307]
SAVE_STATUS_CODES = [200]
IGNORE_STATUS_CODES = [400, 404, 500]
IGNORE_CONTENT_LENGTH = []
RECURSE = true
RECURSE_DEPTH = 5
CUSTOM_EXTENSIONS = ['htm', 'html', 'asp', 'aspx', 'jsp', 'php']
VARIANT_EXTENSIONS = ['bac', 'BAC', 'backup', 'BACKUP', 'bak', 'BAK', 'bk', 'conf', 'cs', 'csproj', 'gz', 'inc', 'INC', 'java', 'log', 'lst', 'old', 'OLD', 'orig', 'ORIG', 'sav', 'save', 'tar', 'temp', 'tmp', 'TMP', 'vb', 'vbproj', 'zip', '$$$', '-OLD', '-old', '0', '1', '~1', '~bk']
IGNORE_EXTENSIONS = ['7z', 'aac', 'bz2', 'class', 'com', 'dmg', 'doc', 'docx', 'exe', 'gif', 'gz', 'iso', 'jar', 'jpeg', 'jpg', 'mp3', 'mpeg', 'mpg', 'pdf', 'pif', 'png', 'ram', 'rar', 'scr', 'snp', 'swf', 'tgz', 'tif', 'tiff', 'wav', 'xls', 'xlsx', 'xml', 'zip']

# functions

def detect_page_not_found(target, hostname)
    conn = true
    ecode = nil
    emesg = nil

    uri = URI(target)
    vhost = hostname || uri.host
    resolve = Ethon::Curl.slist_append(nil, "#{vhost}:#{uri.port}:#{uri.host}")

    random_string = Array.new(8){[*'a'..'z'].sample}.join
    url = "#{target.chomp('/')}/#{random_string}.html"

    method = 'GET'

    request = Typhoeus::Request.new(
        url,
        resolve: resolve,
        method: method,
        followlocation: false,
        connecttimeout: 15,
        timeout: 20,
        ssl_verifyhost: 0,
        ssl_verifypeer: false
    )

    response = request.run

    if not response.timed_out?
        if not response.code.zero?
            # Look for a string we can signature on as well
            if(response.code >= 200 and response.code <= 299)
                HTTP_404_SIGS.each do |sig|
                    if(response.response_body.index(sig))
                        emesg = sig
                        STDERR.puts("[*] - Using custom 404 string of '#{emesg}' - #{target}".blue)
                        break
                    end
                end

                #if(not emesg)
                #    emesg = response.response_body[0,256]
                #    STDERR.puts("[*] Using first 256 bytes of the response as 404 string for #{target}".blue)
                #end
            else
                ecode = response.code
                STDERR.puts("[*] - Using code '#{ecode}' as not found - #{target}".blue)
            end
        end
    end

    return ecode, emesg
end


def brute(target, hostname, wordlist, ecode, emesg)
    results = SortedSet.new

    Typhoeus::Config.user_agent = USER_AGENTS.sample
    hydra = Typhoeus::Hydra.new(max_concurrency: MAX_CONCURRENCY)

    uri = URI(target)
    vhost = hostname || uri.host
    resolve = Ethon::Curl.slist_append(nil, "#{vhost}:#{uri.port}:#{uri.host}")

    max_timeout_counter = 0
    max_error_counter = 0

    wordlist.each do |word|
        url = "#{target.chomp('/')}/#{word}"

        request = Typhoeus::Request.new(
            url,
            resolve: resolve,
            method: METHOD,
            followlocation: false,
            connecttimeout: 15,
            timeout: 20,
            ssl_verifyhost: 0,
            ssl_verifypeer: false
        )

        request.on_complete do |response|
            if response.timed_out?
                max_timeout_counter += 1
                if max_timeout_counter < MAX_TIMEOUTS
                  #STDERR.puts "#{url},TIMEOUT".red
                elsif max_timeout_counter == MAX_TIMEOUTS
                  STDERR.puts "[!] - Too many timeouts encountered, aborting.".red
                  hydra.abort
                else
                end
            elsif response.code.zero?
                # Could not get an http response, something's wrong.
                max_error_counter += 1
                if max_error_counter < MAX_ERRORS
                  #STDERR.puts "#{url},ERROR".red
                elsif max_error_counter == MAX_ERRORS
                  STDERR.puts "[!] - Too many errors encountered, aborting.".red
                  hydra.abort
                else
                end
            else
                next if IGNORE_STATUS_CODES.include? response.code
                next if ((response.code == ecode) or (emesg and response.response_body.index(emesg)))

                content_length = response.headers['content-length'].nil? ? response.body.size : response.headers['content-length']
                next if IGNORE_CONTENT_LENGTH.include? content_length

                results << url if SAVE_STATUS_CODES.include? response.code

                next unless REPORT_STATUS_CODES.empty? or REPORT_STATUS_CODES.include? response.code

                hash = METHOD == 'GET' ? Digest::MD5.hexdigest(response.body) : String.new

                puts "#{url},#{METHOD},#{response.code},#{content_length},#{hash},#{vhost}"
            end
        end

        hydra.queue request
    end

    hydra.run

    results
end


# argument default values 

input = nil
wordlist = nil

# parse arguments

file = __FILE__
ARGV.options do |opts|
    opts.on("-i", "--input FILE", String) { |val| input = val }
    opts.on("-w", "--wordlist FILE", String) { |val| wordlist = val }
    opts.on_tail("-h", "--help")          { exec "grep ^#/<'#{file}'|cut -c4-" }
    opts.parse!
end

# check arguments

if input.nil? then
    puts ARGV.options
    exit 1
end

if not File.exists?(input)
    puts "#{input} does not exist!"
    exit 1
end

if not wordlist.nil? then
    if not File.exists?(wordlist)
        puts "#{wordlist} does not exist!"
        exit 1
    end

    # read in a list of urls to process
    URLS = File.readlines(input).each { |l| l.chomp! }

    URLS.each do |target|
        STDERR.puts "[*] Testing target - #{target}".blue
        target = target.chomp('/')

        ecode, emesg = detect_page_not_found(target, HOST)

        if ecode.nil? and emesg.nil?
            STDERR.puts "[!] 'Page Not Found' identifier could not be found, omitting #{target}".red
            next
        end

        words = IO.readlines(wordlist, :encoding => 'ISO-8859-1').map do |word|
            word = word.strip                       # remove newline and whitespace characters
            word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
            word
        end
        words.uniq!

        results = brute(target, HOST, words, ecode, emesg)
    end
else
    # read in a list of urls to process
    URLS = File.readlines(input).each { |l| l.chomp! }

    URLS.each do |target|
        STDERR.puts "[*] Testing target - #{target}".green

        target = target.chomp('/')

        ecode, emesg = detect_page_not_found(target, HOST)

        if ecode.nil? and emesg.nil?
            STDERR.puts "[!] 'Page Not Found' identifier could not be found, omitting #{target}".red
            next
        end

        # 1) Test directory list

        dir_results = SortedSet.new

        dirs = IO.readlines(DIR_LIST, :encoding => 'ISO-8859-1').map do |word|
            word = word.strip                       # remove newline and whitespace characters
            word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
            word += '/' if word[-1,1] != '/'        # add trailing slash if it doesn't exist
            word
        end
        dirs.uniq!

        urls = SortedSet.new
        urls << target
        depth = 0

        loop do
            results = SortedSet.new
            depth += 1

            urls.each do |url|
                STDERR.puts "[*] - Testing directory list - #{url}".blue
                results += brute(url, HOST, dirs, ecode, emesg)
            end

            break if results.empty?
            break unless RECURSE
            break if depth == RECURSE_DEPTH

            dir_results += results
            urls = results
        end

        # 2) Test file list with no extensions

        files = IO.readlines(FILE_LIST, :encoding => 'ISO-8859-1').map do |word|
            word = word.strip                       # remove newline and whitespace characters
            word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
            word = word.chomp('/')                  # remove trailing slash if it exists
            word
        end
        files.uniq!

        dir_results << target

        file_results = SortedSet.new

        dir_results.each do |url|
            STDERR.puts "[*] - Testing file list with no extensions - #{url}".blue
            file_results += brute(url, HOST, files, ecode, emesg)
        end

        # 3) Test file list with custom extensions

        files = IO.readlines(FILE_LIST, :encoding => 'ISO-8859-1').flat_map do |word|
            word = word.strip                       # remove newline and whitespace characters
            word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
            combi = Array.new
            CUSTOM_EXTENSIONS.each do |ext|
                combi << word + '.' + ext
            end
            combi
        end
        files.uniq!

        dir_results.each do |url|
            STDERR.puts "[*] - Testing file list with custom extensions - #{url}".blue
            file_results += brute(url, HOST, files, ecode, emesg)
        end

        # 4) Test variant extensions on discovered files

        files = SortedSet.new
        file_results.each do |url|
            uri = URI(url)
            VARIANT_EXTENSIONS.each do |ext|
                file = uri.path + '.' + ext
                file = file[1..-1] if file[0,1] == '/'  # remove leading slash if it exists
                files << file
            end
        end

        unless files.empty?
            STDERR.puts "[*] - Testing variant extensions on discovered files".blue
            file_results += brute(target, HOST, files, ecode, emesg)
        end

        # 5) Test file list with observed extensions (if different from custom/variant), except for (bz2, class, com, doc, docx, exe, gif, gz, jar, jpeg, jpg, mp3, mpeg, mpg, pdf, pif, png, ram, rar, scr, snp, swf, tgz, tif, wav, xls, xlsx, xml, zip).
        # 6) Test observed files with custom extensions.
        # 7) Test observed files with variant extensions.
        # 8) Test observed files with observed extensions, except for (bz2, class, com, doc, docx, exe, gif, gz, jar, jpeg, jpg, mp3, mpeg, mpg, pdf, pif, png, ram, rar, scr, snp, swf, tgz, tif, wav, xls, xlsx, xml, zip).

        # Options:
        # A) Extract links
        # B) Files only, directories only, both files and directories
        # C) Recurse sub-directories (to depth: X)

        # Input: List of URLs

        # Group URLs by IP + hostname, so that enumeration does not happen twice
        # Split each batch of tests into tasks, and add to a queue
        # Tasks can then be pushed to top or bottom of queue as appropiate
    end
end
